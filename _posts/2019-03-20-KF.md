---
layout: post
title: 基于滤波的状态估计（一）线性高斯系统与卡尔曼滤波（Kalman Filter, KF）
category: 知识整理
tags: 
  - SLAM
---

## 1. 问题描述

在SLAM问题中，我们通常假设机器人携带着传感器在一定的未知环境中运动，同时进行着定位与建图的任务。定位可以帮助机器人识别到自己所处的位置，进而对后续任务的执行起到引导的作用，而建图中机器人将记录下一系列的路标，可以作为观测（obsevation）辅助定位，也可以用于减少漂移（loop closure）或是让机器人在下一次执行任务时得到同样坐标系下的坐标（relocalization）。传感器通常是在一定的时间节点进行数据采集，我们也只关心这些时刻的状态和地图，所以SLAM的问题通常简化为离散时刻的任务 $t = 1, \dots ,K$， 用 $x$ 表示机器人的位置状态，于是各个时刻的位置标记为 $x_1, \dots, x_K$， 这些位置组成了机器人的轨迹。同时，我们假设地图是由一个个路标（landmark）组成，每个时刻，传感器都能够观测到一部分路标，得到观测数据，设每个时刻观测到的数据为 $y_1, \dots y_K$。实际上，SLAM中的定位是通过**运动**和**观测**来来描述的，如果这两者发生在线性高斯系统（Linear-Gaussian, LG）下，则我们可以建立如下的方程：


$$
\begin{align}
\text{Motion Model:} \quad & x_k = A_{k-1} x_{k-1} + v_k + w_k, \quad k = 1, \dots, K \\
\text{Observation Model: } \quad & y_k = C_k x_k + n_k, \quad k = 0, \dots, K
\end{align}
$$


其中 $A_{k-1}$ 成为转移矩阵（transition matrix），代表状态从 $k-1$ 时刻到 $k$ 时刻的变化，$v_k$ 是系统的输入，$w_k$ 和 $n_k$ 分别是运动模型和观测模型的噪声，各个变量的性质如下：


$$
\begin{align}
\text{system state:} \quad & x_k \in \mathbb{R}^N \\
\text{initial state:} \quad & x_0  \in \mathbb{R}^N \sim \mathcal{N}(\check{x}_0, \check{P}_0) \\
\text{input: } \quad & v_k  \in \mathbb{R}^N  \\
\text{process noise: } \quad & w_k  \in \mathbb{R}^N \sim \mathcal{N}(0, Q_k) \\
\text{measurement: } \quad & y_k  \in \mathbb{R}^N \\
\text{measurement noise: } \quad & n_k  \in \mathbb{R}^N \sim \mathcal{N}(0, R_k)
\end{align}
$$

若在求解系统某一时刻的状态 $k$ 时，使用了系统的全部时刻的数据（$v_1, \dots, v_K, y_1, \dots , y_K$），则称为离散时间的批量估计，这里暂不展开。批量估计综合了系统中所有时刻的数据，显然更为准确，但对于时刻 $k$ 来说，由于使用了未来的数据；所以批量估计的方法是无法做到实时的。更为常见的是，我们通过时刻 $1$ 到时刻 $k-1$ 的状态和时刻 $1$ 到时刻 $k$ 的观测来估计时刻 $k$ 的状态，称为离散时间的滤波算法。滤波算法没有利用未来的信息，所以可以做到实时估计，但由于每一步都用到了前面所有的信息，所以会导致效率低下，若我们假设系统具有一阶马尔科夫性质，即 $k$ 时刻的状态只取决于 $k-1$ 时刻的状态和 $k$ 时刻的输入，且 $k$ 时刻的观测仅取决于 $k$ 时刻的状态。在这一假设下，我们可以利用 $k-1$ 时刻的估计以及 $k$ 时刻的输出和观测来估计 $k$ 时刻的状态，这就是离散时间的迭代滤波，也叫做卡尔曼滤波（Kalman Filter），如下图所示：

![][1]





问题的求解通常有着两种思路：

1. 最大后验估计（Maximum a posteriori, MAP）：通过优化的理论，来寻找给定信息下的最大后验概率，从而求得极值处系统的状态
2. 贝叶斯推断（Bayesian inference）：从状态的先验概率密度函数出发，通过初始状态、输入、运动方程和观测推导出后验概率密度函数，从函数的形式上求得系统的状态

无论是哪种方法，我们都需要求解时刻 $k$ 的后验概率分布，如下所示：



![][2]





图中的 $\hat{x}'_{k-1}$ 代表用 $0$ 到 $k$ 时刻的数据估算出的 $k-1$ 时刻的状态， Kalman Filter 属于迭代滤波，也就是用 $k-1$ 时刻的状态来估计 $k$ 时刻的状态，如下所示：


$$
\lbrace  \hat{x}_{k-1}, \hat{P}_{k-1} \rbrace \to \lbrace  \hat{x}_{k}, \hat{P}_{k} \rbrace
$$



**Notation**

- 最大后验估计关注的是后验概率极值点，而贝叶斯推断关注的时后验概率的期望值点
- 在线性高斯系统中，这两者是一致的




## 2. 从最大后验估计推导卡尔曼滤波



在推导卡尔曼滤波之前，我们先用贝叶斯法则来重写 $x_k$ 的后验概率表达式：


$$
\begin{align}
p(x_k \mid \hat{x}_{k-1}, v_k, y_k) & = \eta p (y_k \mid x_k, \hat{x}_{k-1}, v_k) p(x_k \mid \hat{x}_{k-1}, v_k) \\
& = \eta  p (y_k \mid x_k) p(x_k \mid {x}'_{k-1}, v_k) p({x}'_{k-1} \mid \hat{x}_{k-1})
\end{align}
$$


这里的 $\eta$ 是归一化常数， $x'_{k-1}$ 是指 $0$ 至 $k$ 时刻数据对 $k-1$ 时刻状态的估计，这里的重写实际上用到了马尔科夫性质（Markov property），在LG系统中，这一猜想认为是满足的。MAP是求其后验概率的极值，为了简化运算，我们将后验概率写成对数形式：


$$
ln p(x_k \mid \hat{x}_{k-1}, v_k, y_k) = ln ( p (y_k \mid x_k)) + ln( p(x_k \mid x'_{k-1}, v_k)) + ln ( p(x'_{k-1} \mid \hat{x}_{k-1})) + C
$$


这里及之后的 $C$ 代表与 $x$ 无关的常数。由之前的定义可知：


$$
\begin{align}
& ln ( p (y_k \mid x_k)) = -\frac{1}{2} (y_k - C_k {x}_k)^T R_k^{-1} (y_k - C_k {x}_k) + \underbrace{ -\frac{1}{2} ln((2 \pi)^N det R_k )}_{\text{independent of x}} \\
& ln ( p(x_k \mid {x}'_{k-1}, v_k)) = -\frac{1}{2} (x_k - A_{k-1} {x}'_{k-1})^T Q_k^{-1} (x_k - A_{k-1} {x}'_{k-1}) + \underbrace{ -\frac{1}{2} ln((2 \pi)^N det Q_k )}_{\text{independent of x}} \\
& ln ( p({x}'_{k-1} \mid \hat{x}_{k-1})) = -\frac{1}{2} (x'_{k-1} - \hat{x}_{k-1})^T \hat{P}_{k-1}^{-1} (x'_{k-1} - \hat{x}_{k-1}) + \underbrace{ -\frac{1}{2} ln((2 \pi)^N det \hat{P}_{k-1} )}_{\text{independent of x}} \\
\end{align}
$$


这些都是平方马氏距离（Mahalanobis distance），忽略和 $x$ 无关的项，我们可以构建整体的目标函数：


$$
J(x) =  (y_k - C_k {x}_k)^T R_k^{-1} (y_k - C_k {x}_k) + (x_k - A_{k-1} {x}'_{k-1})^T Q_k^{-1} (x_k - A_{k-1} {x}'_{k-1}) + (x'_{k-1} - \hat{x}_{k-1})^T \hat{P}_{k-1}^{-1} (x'_{k-1} - \hat{x}_{k-1})
$$


后验概率最大处也就是目标函数 $J(x)$ 的最小值处。接着，定义如下变量：


$$
z = \left\lbrack \begin{matrix} \hat{x}_{k-1} \\ v_k \\ y_k \end{matrix} \right\rbrack , \quad H =  \left\lbrack \begin{matrix} 1 &  \\ -A_{k-1} & 1 \\  & C_k \end{matrix} \right\rbrack, \quad W = \left\lbrack \begin{matrix} \hat{P}_{k-1} & & \\  & Q_k &  \\ & & R_k \end{matrix} \right\rbrack, \quad {x} = \left\lbrack \begin{matrix} {x}'_{k-1} \\ {x}_k   \end{matrix} \right\rbrack
$$


则目标函数可以写成如下形式：



$$
J(x) = (z - Hx)^T W^{-1} (z - Hx) \\
\hat{x}_k = \left\lbrack \begin{matrix} \hat{x}'_{k-1} \\ \hat{x}_k   \end{matrix} \right\rbrack = \mathop{\arg\min}_{x_k} J(x) 
$$


$J(x)$ 是一个抛物面，最小值是其偏导数为0处的值：


$$
\left. \frac{\partial J(x)}{\partial x^T} \right\vert_{\hat{x}} = -H^TW^{-1}(z - H\hat{x}) = 0 \\
(H^T W^{-1} H) \hat{x} = H^T W^{-1} z
$$


代入 $z,H,W$，可得:


$$
\left\lbrack \begin{matrix} \hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1} & -A^T_{k-1}Q^{-1}_k  \\ -Q^{-1}_k A_{k-1} & Q^{-1}_k + C^T_k R^{-1}_k C_k   \end{matrix} \right\rbrack \left\lbrack \begin{matrix} \hat{x}'_{k-1} \\ \hat{x}_k   \end{matrix} \right\rbrack = \left\lbrack \begin{matrix} \hat{P}^{-1}_{k-1}\hat{x}_{k-1} -A^T_{k-1}Q^{-1}_k v_k   \\ Q^{-1}_kv_k + C^T_k R^{-1}_k y_k  \end{matrix} \right\rbrack
$$


在求解的过程中，我们其实并不关心 $\hat{x}'_{k-1}$ 的解，利用公式 (6.1.1)，左乘如下矩阵：


$$
\left\lbrack \begin{matrix} I_p & 0 \\ Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1} & I_p \end{matrix} \right\rbrack
$$


方程变为：


$$
\begin{aligned}
& \left\lbrack \begin{matrix} \hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1} & -A^T_{k-1}Q^{-1}_k  \\ 0 & Q^{-1}_k + C^T_k R^{-1}_k C_k - Q^{-1}_k A_{k-1} (\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1} A^T_{k-1}Q^{-1}_k  \end{matrix} \right\rbrack \left\lbrack \begin{matrix} \hat{x}'_{k-1} \\ \hat{x}_k   \end{matrix} \right\rbrack \\
= &\left\lbrack \begin{matrix} \hat{P}^{-1}_{k-1}\hat{x}_{k-1} -A^T_{k-1}Q^{-1}_k v_k   \\ Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}(\hat{P}^{-1}_{k-1}\hat{x}_{k-1} -A^T_{k-1}Q^{-1}_k v_k) + Q^{-1}_kv_k + C^T_k R^{-1}_k y_k  \end{matrix} \right\rbrack
\end{aligned}
$$


将矩阵乘法的第二行提出来，便是 $\hat{x}_k$ 的解：


$$
\begin{aligned}
& \underbrace{(Q^{-1}_k + C^T_k R^{-1}_k C_k - Q^{-1}_k A_{k-1} (\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1} A^T_{k-1}Q^{-1}_k) }_{\text{待化简}} \hat{x}_k \\
= &Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}(\hat{P}^{-1}_{k-1}\hat{x}_{k-1} -A^T_{k-1}Q^{-1}_k v_k) + Q^{-1}_kv_k + C^T_k R^{-1}_k y_k
\end{aligned}
$$


利用公式 (6.2.2)，可以化简等式的左侧：


$$
\begin{align}
& Q^{-1}_k + C^T_k R^{-1}_k C_k - Q^{-1}_k A_{k-1} (\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1} A^T_{k-1}Q^{-1}_k \\
= & Q^{-1}_k - Q^{-1}_k A_{k-1} (\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1} A^T_{k-1}Q^{-1}_k +  C^T_k R^{-1}_k C_k \\
= & \underbrace{ (Q_k + A_{k-1}\hat{P}_{k-1}A_{k-1}^T)^{-1} + C^T_k R^{-1}_k C_k }_{\hat{P}_k^{-1},\text{ 定义见下} }
\end{align}
$$


此时，我们定义如下变量：


$$
\begin{align}
& \check{P}_k = Q_k + A_{k-1}\hat{P}_{k-1}A_{k-1}^T \\
& \hat{P}_k = (\check{P}_k^{-1} + C^T_k R^{-1}_k C_k)^{-1}
\end{align}
$$


代入 $\hat{x}_k$ 的解：


$$
\begin{align}
\hat{P}_k^{-1} \hat{x}_k & = Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}(\hat{P}^{-1}_{k-1}\hat{x}_{k-1} -A^T_{k-1}Q^{-1}_k v_k) + Q^{-1}_kv_k + C^T_k R^{-1}_k y_k \\
& = \underbrace{ Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}\hat{P}^{-1}_{k-1} }_{\check{P}^{-1}_k A_{k-1}, \text{证明见下}} \hat{x}_{k-1} \\
& + \underbrace{( -Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}A^T_{k-1}Q^{-1}_k + Q^{-1}_k)}_{\check{P}_k^{-1}, \text{ 代入公式（6.2.2）可直接得到定义}} v_k + C^T_k R^{-1}_k y_k \\
& = \check{P}_k^{-1}\underbrace{(A_{k-1}\hat{x}_{k-1} + v_k)}_{\check{x}_k} + C^T_k R^{-1}_k y_k 
\end{align}
$$


上式中第一步化简的证明如下：


$$
\begin{align}
Q^{-1}_k A_{k-1} \underbrace{ (\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}}_{\text{代入公式 （6.2.2）}} \hat{P}^{-1}_{k-1} & = Q^{-1}_k A_{k-1}(\hat{P}_{k-1} - \hat{P}_{k-1}A^T_{k-1}\underbrace{(Q_k + A_{k-1}\hat{P}_{k-1}A_{k-1}^T)^{-1}}_{\check{P}_k^{-1}} A_{k-1}\hat{P}_{k-1} )\hat{P}^{-1}_{k-1} \\
& =  Q^{-1}_k A_{k-1}(\hat{P}_{k-1} - \hat{P}_{k-1}A^T_{k-1}\check{P}_{k-1}^{-1} A_{k-1} \hat{P}_{k-1} )\check{P}_{k-1}^{-1} \\
& =  (Q^{-1}_k - Q^{-1}_k \underbrace{A_{k-1} \hat{P}_{k-1}A^T_{k-1}}_{\check{P}_k - Q_k, \text{ 定义}}\check{P}_{k-1}^{-1}) A_{k-1}   \\
& = (Q^{-1}_k - Q^{-1}_k + \check{P}^{-1}_k)A_{k-1} \\
& = \check{P}^{-1}_k A_{k-1}
\end{align}
$$


至此，我们已推出了基于MAP的迭代滤波的解：


$$
\begin{matrix}
\text{Prediction:} & \begin{matrix} \check{P}_k = A_{k-1}\hat{P}_{k-1}A_{k-1}^T + Q_k \\ \check{x}_k = A_{k-1}\hat{x}_{k-1} + v_k \end{matrix} \\
\text{Correction} & \begin{matrix} \hat{P}_k^{-1} =\check{P}_k^{-1} + C^T_k R^{-1}_k C_k \\ \hat{P}_k^{-1}\hat{x}_k = \check{P}_k^{-1}\check{x}_k + C^T_k R^{-1}_k y_k  \end{matrix} 
\end{matrix}
$$


定义卡尔曼增益：


$$
K_k = \hat{P}_k C_k^T R_k^{-1}
$$


此时，卡尔曼增益中含有 $\hat{P}_k$ 项，我们进行如下变换：


$$
\begin{align}
1 & = \hat{P}_k (\check{P}_k^{-1} + C^T_k R^{-1}_k C_k) \\
& = \hat{P}_k \check{P}_k^{-1} + K_k C_k \\
\hat{P}_k  & = (1 - K_k C_k) \check{P}_k \\
\underbrace{\hat{P}_k C_k^T R_k^{-1}}_{K_k} & = (1 - K_k C_k) \check{P}_k C_k^T R_k^{-1} \\
K_k (1 + C_k \check{P}_k C_k^T R_k^{-1} ) & = \check{P}_k C_k^T R_k^{-1}\\
K_k &= \check{P}_k C_k^T (C_k \check{P}_k C_k^T + R_k )^{-1}
\end{align}
$$


将其代入迭代滤波的解：


$$
\begin{align}
\hat{P}_k^{-1} &=\check{P}_k^{-1} + C^T_k R^{-1}_k C_k \\
1 & = \hat{P}_k \check{P}_k^{-1} + \underbrace{\hat{P}_k C^T_k R^{-1}_k}_{K_k} C_k \\
\hat{P}_k \check{P}_k^{-1} & = 1 - K_k C_k \\
\hat{P}_k & = (1 - K_k C_k) \check{P}_k^{-1}
\end{align}
$$

$$
\begin{align}
\hat{P}_k^{-1}\hat{x}_k & = \check{P}_k^{-1}\check{x}_k + C^T_k R^{-1}_k y_k \\
\hat{x}_k & = \underbrace{\hat{P}_k \check{P}_k^{-1}}_{1-K_kC_k}\check{x}_k + \underbrace{\hat{P}_k C^T_k R^{-1}_k}_{K_k} y_k \\
\hat{x}_k & = \check{x}_k + K_k(y_k - C_k \check{x}_k)
\end{align}
$$




便得到了经典的卡尔曼滤波：


$$
\begin{matrix}
\text{Prediction:} & \begin{matrix} \check{P}_k = A_{k-1}\hat{P}_{k-1}A_{k-1}^T + Q_k \\ \check{x}_k = A_{k-1}\hat{x}_{k-1} + v_k \end{matrix} \\
\text{Kalman gain:} & K_k = \check{P}_k C_k^T (C_k \check{P}_k C_k^T + R_k )^{-1} \\
\text{Correction} & \begin{matrix} \hat{P}_k =  (1-K_k C_k)\check{P}_k^{-1} \\ \hat{x}_k = \check{x}_k + K_k(y_k - C_k \check{x}_k)  \end{matrix} 
\end{matrix}
$$






## 3. 从贝叶斯推断推导卡尔曼滤波

相比于基于MAP的推导，基于贝叶斯推断来推导卡尔曼滤波要较为简单和清晰。首先，进行如下定义：


$$
p(x_{k-1} \vert \check{x}_0, v_{1:k-1}, y_{0:k-1}) = \mathcal{N}\left( \hat{x}_{k-1}, \; \hat{P}_{k-1} \right)
$$


此处定义了 $k-1$ 时刻系统的后验概率分布。接着，针对预测部分，引入 $v_k$，写成先验形式：


$$
p({x}_{k} \vert \check{x}_0, v_{1:k}, y_{0:k-1}) = \mathcal{N}\left( \check{x}_{k-1}, \check{P}_{k-1} \right)
$$


对于先验概率的均值和方差，我们有：


$$
\begin{align}
\check{x}_k & = E \lbrack x_k \rbrack = E \lbrack A_{k-1}x_{k-1} + v_k + w_k \rbrack \\
& = A_{k-1} \underbrace{E \lbrack x_{k-1} \rbrack}_{\hat{x_{k-1}}} + v_k + \underbrace{E \lbrack w_k \rbrack}_0 \\
& = A_{k-1} \hat{x}_{k-1} + v_k
\end{align}
$$

$$
\begin{align}
\check{P}_k & = E \left\lbrack (x_k -  E \lbrack x_k \rbrack)(x_k -  E \lbrack x_k \rbrack)^T \right\rbrack \\
& = E \left\lbrack (A_{k-1}x_{k-1} + v_k + w_k - A_{k-1} \hat{x}_{k-1} - v_k ) (A_{k-1}x_{k-1} + v_k + w_k - A_{k-1} \hat{x}_{k-1} - v_k )^T\right\rbrack \\
& = A_{k-1} \underbrace{ E \left\lbrack (x_{k-1} - \hat{x}_{k-1})(x_{k-1} - \hat{x}_{k-1})^T \right\rbrack}_{\hat{P}_{k-1}} A_{k-1}^T + \underbrace{E\left\lbrack w_kw_k^T \right\rbrack }_{Q_k} + 2 \underbrace{E\left\lbrack  A_{k-1}(x_{k-1} - \hat{x}_{k-1})  \right\rbrack  E\left\lbrack w_k^T \right\rbrack }_0 \\
& = A_{k-1} \hat{P}_{k-1} A_{k-1}^T + Q_k
\end{align}
$$


此时，卡尔曼滤波的**预测**部分如下：


$$
\begin{align}
& \check{x}_k = A_{k-1} \hat{x}_{k-1} + v_k \\
& \check{P}_k =  A_{k-1} \hat{P}_{k-1} A_{k-1}^T + Q_k
\end{align}
$$


同理，不难得出：


$$
\begin{align}
& E \left\lbrack (x_k -  E \lbrack x_k \rbrack)(y_k -  E \lbrack y_k \rbrack)^T \right\rbrack =  E \left\lbrack (x_k -  E \lbrack x_k \rbrack)(x_k -  E \lbrack x_k \rbrack + v_k)^T \right\rbrack C_k^T = \check{P_k}C_k^T \\
& E \left\lbrack(y_k -  E \lbrack y_k \rbrack) (x_k -  E \lbrack x_k \rbrack)^T \right\rbrack =  C_k E \left\lbrack (x_k + v_k -  E \lbrack x_k \rbrack)(x_k -  E \lbrack x_k \rbrack )^T \right\rbrack  = C_k \check{P_k} \\
& E \left\lbrack(y_k -  E \lbrack y_k \rbrack) (y_k -  E \lbrack y_k \rbrack)^T \right\rbrack =  C_k E \left\lbrack (x_k + v_k -  E \lbrack x_k \rbrack)(x_k -  E \lbrack x_k \rbrack + v_k )^T \right\rbrack C_k^T = C_k \check{P_k} C_k^T + R_k
\end{align}
$$


所以，我们可以写出 $(x_k, y_k)$ 的联合概率分布：


$$
\begin{align}
p(x_k, y_k \vert \check{x}_0, v_{1:k}, y_{0:k-1}) & = \mathcal{N} \left( \left\lbrack \begin{matrix} \mu_x \\ \mu_y \end{matrix} \right\rbrack,\; \left\lbrack \begin{matrix} \Sigma_{xx} & \Sigma_{xy} \\ \Sigma_{yx} & \Sigma_{yy} \end{matrix} \right\rbrack \right) \\
& = \mathcal{N} \left( \left\lbrack \begin{matrix} \check{x}_k \\ C_k\check{x}_k \end{matrix} \right\rbrack,\; \left\lbrack \begin{matrix} \check{P_k} & \check{P_k} C_k^T \\ C_k \check{P_k}& C_k \check{P_k} C_k^T + R_k \end{matrix} \right\rbrack \right)
\end{align}
$$


由公式 （6.3.2） 可知：


$$
\begin{align}
p(x_k \vert \check{x}_0, v_{1:k}, y_{0:k}) & = \mathcal{N} (\underbrace{ \mu_x + \Sigma_{xy}\Sigma_{yy}^{-1}(y_k - \mu_y)}_{\hat{x}_k} ,\; \underbrace{\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}}_{\hat{P}_k} ) \\
& = \mathcal{N} (\check{x}_k + \check{P_k}C_k^T(C_k\check{p}_kC_k^T + R_k)^{-1}(y_k - C_k\check{x_k})  ,\; \check{P}_k - \check{P}_k C_k^T(C_k\check{p}_kC_k^T + R_k)^{-1}C_k\check{P}_k ) \\
& = \mathcal{N} ( \check{x}_k + \underbrace{\check{P_k}C_k^T(C_k\check{p}_kC_k^T + R_k)^{-1}}_{K_k}(y_k - C_k\check{x_k}) ,\; ( 1 - \underbrace{\check{P_k}C_k^T(C_k\check{p}_kC_k^T + R_k)^{-1}}_{K_k} C_k) \check{P}_k)
\end{align}
$$


此时，我们可以得到**卡尔曼增益**和**校正**部分：


$$
\begin{align}
& K_k = \check{P_k}C_k^T(C_k\check{p}_kC_k^T + R_k)^{-1} \\
& \hat{x}_k = \check{x}_k + K_k(y_k - C_k\check{x}_k) \\
& \hat{P}_k = (1 - K_k C_k)\check{P}_k
\end{align}
$$




## 4. 从增益的角度看卡尔曼增益 

通常，我们认为卡尔曼滤波是最优的，虽然从 MAP 和 Bayesian Inference 中推出了相关公式，但对于最优这个特性并没有很好的表示，这一部分通过增益的角度，来看待卡尔曼滤波的最优特性，定义一个估计器如下：


$$
\hat{x}_k = \check{x}_k + K_k(y_k - C_k\check{x}_k)
$$


误差为：


$$
\begin{align}
\hat{e}_k & = \hat{x}_k - x_k = \check{x}_k + K_k(y_k - C_k\check{x}_k) - x_k \\
& = (1- K_k C_k)  \check{x}_k + K_k \underbrace{ y_k}_{C_kx_k + n_k} - x_k \\
& = (1- K_k C_k) (\check{x}_k  - x_k) + K_k n_k
\end{align}
$$


所以有：


$$
\begin{align}
E \left\lbrack \hat{e}_k \hat{e}_k^T \right\rbrack & = (1- K_k C_k) E \left\lbrack (\check{x}_k  - x_k) (\check{x}_k  - x_k) ^T \right\rbrack (1- K_k C_k)^T + K_k E \left\lbrack n_k n_k^T \right\rbrack K_k \\
& = (1- K_k C_k) \check{P}_k (1- K_k C_k)^T + K_k R_k K_k^T
\end{align}
$$


定义一个代价函数：



$$
J(K_k) = \frac{1}{2} tr E \left\lbrack \hat{e}_k \hat{e}_k^T \right\rbrack  = E \left\lbrack \frac{1}{2} tr \hat{e}_k^T \hat{e}_k \right\rbrack
$$



由公式 （6.4.1）和（6.4.2）可知：


$$
\frac{\partial J(K_k)}{\partial K_k} = -(1 - K_k C_k) \check{P}_k C_k^T + K_k R_k
$$


$J(K_k) $ 的最小处也就是导数的零点，此时：


$$
K_k = \check{P_k}C_k^T(C_k\check{p}_kC_k^T + R_k)^{-1}
$$


这里可以看出，卡尔曼增益的取值使得估计器的估计误差最小



## 5. 卡尔曼滤波的性质讨论

在这一节，将讨论卡尔曼滤波器为什么是最优无偏的滤波器，定义如下误差：


$$
\check{e}_k = \check{x}_k - x_k \\
\hat{e}_k = \hat{x}_k - x_k
$$


由卡尔曼的预测和修正方程可知：


$$
\begin{align}
& \check{e}_k = \check{x}_k - x_k = (A_{k-1} \hat{x}_{k-1} + v_k) - (A_{k-1} {x}_{k-1} + v_k + w_k) = A_{k-1}\hat{e}_{k-1} + w_k \\
& \hat{e}_k = \hat{x}_k - x_k = \check{x}_k + K_k(y_k - C_k\check{x}_k) - x_k = (1 - K_k C_k)\check{x}_k + K_k (C_k x_k + n_k) - x_k = (1-K_kC_k)\check{e}_k + K_kn_k
\end{align}
$$



显然， $E[\hat{e}_0] = 0$，可用数学归纳法依次递推：


$$
\begin{align}
& E[\check{e}_k] = A_{k-1} \underbrace{E[\hat{e}_{k-1}] }_0 - \underbrace{E[w_k]}_0 = 0 \\
& E[\hat{e}_k] =  (1-K_kC_k)\underbrace{E[\check{e}_k]}_0 + K_k \underbrace{E[n_k]}_0 = 0
\end{align}
$$


此处可以证明滤波器的误差均值为零，是无偏的滤波器。同时，我们也可以证明滤波器的误差的均方差便是给出的 $\check{P}_k$ 和 $\hat{P}_k$ :


$$
\begin{align}
E[\check{e}_k \check{e}_k^T] & = E \left\lbrack (A_{k-1}\hat{e}_{k-1} + w_k )(A_{k-1}\hat{e}_{k-1} + w_k )^T \right\rbrack \\
& = A_{k-1} \underbrace{E[\hat{e}_{k-1} \hat{e}_{k-1}^T]}_{\hat{P}_{k-1}} A_{k-1}^T + \underbrace{E[w_k w_k^T]}_{Q_k} + A_{k-1} \underbrace{E [\hat{e}_{k-1}  w_k^T]}_0 + \underbrace{E [\hat{e}_{k-1}^T  w_k]}_0 A_{k-1} ^T \\
& = \check{P}_k \\
E[\hat{e}_k \hat{e}_k^T] & = (1-K_kC_k)  \underbrace{E[\check{e}_{k} \check{e}_{k}^T]}_{\check{P}_{k}} (1-K_kC_k)^T + K_k  \underbrace{E[n_kn_k^T]}_{R_k} K_k^T + 0 \\
& = \underbrace{(1-K_kC_k)\check{P}_{k}}_{\hat{P}_k}(1 - C_k^T K_k^T) + K_k R_k K_k^T \\
& = \hat{P}_k - \underbrace{\hat{P}_k C_k^T K_k^T + K_k R_k K_k^T}_{0 \text{ because } K_k = \hat{P}_k C_k^T R_k^{-1}} \\
& = \hat{P}_k 
\end{align}
$$


综上，卡尔曼滤波器是线性高斯系统中的最优无偏估计


## 6. 数学补充

#### 6.1 舒尔补（Schur complement）

对于一块矩阵 $M$  :
$$
M = \left\lbrack \begin{matrix} A & B \\ C & D \end{matrix} \right\rbrack
$$

若 $A$ 是可逆矩阵，则 $M$ 对 $A$ 的舒尔补为： $M/A = D - CA^{-1}B$

若 $D$ 是可逆矩阵，则 $M$ 对 $D$ 的舒尔补为： $M/D = A - BD^{-1}C$  

舒尔补可以用于构造上三角矩阵：

$$
\begin{align}
\text{左乘构造上三角} \quad & \left\lbrack \begin{matrix} I_p &  \\ -CA^{-1} & I_p \end{matrix} \right\rbrack \left\lbrack \begin{matrix} A & B \\ C & D \end{matrix} \right\rbrack = \left\lbrack \begin{matrix} A & B \\ 0 & D - CA^{-1}B \end{matrix} \right\rbrack \tag{6.1.1}\\
\text{右乘构造上三角} \quad & \left\lbrack \begin{matrix} A & B \\ C & D \end{matrix} \right\rbrack \left\lbrack \begin{matrix} I_p &  \\ -D^{-1}C & I_p \end{matrix} \right\rbrack = \left\lbrack \begin{matrix} A - BD^{-1}C & B \\ 0 & D \end{matrix} \right\rbrack \tag{6.1.2}
\end{align}
$$

#### 6.2 SMW等价（Sherman-Morrison-Woodbury Identity）

对如下矩阵做LDU分解（lower-diagonal-upper）和UDL分解（upper-diagonal-lower）：

$$
\begin{align}
\left\lbrack \begin{matrix} A^{-1} & -B \\ C & D \end{matrix} \right\rbrack & = \left\lbrack \begin{matrix} 1 & 0 \\ CA & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix} A^{-1} & 0 \\ 0 & D + CAB \end{matrix} \right\rbrack \left\lbrack \begin{matrix} 1 & -AB \\ 0 & 1 \end{matrix} \right\rbrack \quad (LDU) \\
& = \left\lbrack \begin{matrix} 1 & -BD^{-1} \\ 0 & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix} A^{-1} + BD^{-1}C & 0 \\ 0 & D \end{matrix} \right\rbrack \left\lbrack \begin{matrix} 1 & 0 \\ D^{-1}C & 1 \end{matrix} \right\rbrack \quad (UDL)
\end{align}
$$

针对LDU和UDL分别进行矩阵求导，可得：

$$
\begin{align}
\left\lbrack \begin{matrix} A^{-1} & -B \\ C & D \end{matrix} \right\rbrack^{-1} & = \left\lbrack \begin{matrix} 1 & AB \\ 0 & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix} A & 0 \\ 0 & (D + CAB)^{-1} \end{matrix} \right\rbrack \left\lbrack \begin{matrix} 1 & 0 \\ -CA & 1 \end{matrix} \right\rbrack  \\
& = \left\lbrack \begin{matrix} A-AB(D+CAB)^{-1}CA & AB(D+CAB)^{-1} \\ -(D+CAB)^{-1}CA & (D+CAB)^{-1} \end{matrix} \right\rbrack \quad (LDU) \\
& = \left\lbrack \begin{matrix} 1 & 0 \\ -D^{-1}C & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix}(A^{-1} + BD^{-1}C)^{-1} & 0 \\ 0 & D^{-1} \end{matrix} \right\rbrack \left\lbrack \begin{matrix} 1 & BD^{-1} \\ 0 & 1 \end{matrix} \right\rbrack \\
& = \left\lbrack \begin{matrix} (A^{-1} + BD^{-1}C)^{-1} & (A^{-1} + BD^{-1}C)^{-1}BD^{-1} \\ -D^{-1}C(A^{-1} + BD^{-1}C)^{-1} & D^{-1} - D^{-1}C(A^{-1} + BD^{-1}C)^{-1}BD^{-1} \end{matrix} \right\rbrack \quad (UDL)
\end{align}
$$

实际上两个求逆后的矩阵应该是相等的，只是由于分解的顺序不同，所以以不同的形式呈现了出来，因此可以得到非常重要的SMW等价：

$$
\begin{align}
(A^{-1} + BD^{-1}C)^{-1} & \equiv A-AB(D+CAB)^{-1}CA \tag{6.2.1} \\
(D+CAB)^{-1} & \equiv D^{-1} - D^{-1}C(A^{-1} + BD^{-1}C)^{-1}BD^{-1} \tag{6.2.2} \\
AB(D+CAB)^{-1} & \equiv (A^{-1} + BD^{-1}C)^{-1}BD^{-1} \tag{6.2.3} \\
(D+CAB)^{-1}CA & \equiv D^{-1}C(A^{-1} + BD^{-1}C)^{-1} \tag{6.2.4}
\end{align}
$$


#### 6.3 联合高斯概率密度函数

对于两个高斯变量 $(x, y)$，其联合概率密度函数如下：

$$
p(x,y) = \mathcal{N} \left( \left\lbrack \begin{matrix} \mu_x \\ \mu_y \end{matrix} \right\rbrack,\; \left\lbrack \begin{matrix} \Sigma_{xx} & \Sigma_{xy} \\ \Sigma_{yx} & \Sigma_{yy} \end{matrix} \right\rbrack \right) \tag{6.3.1}
$$

利用 6.2 中介绍的UDL分解，可以对其方差矩阵进行如下分解：

$$
\left\lbrack \begin{matrix} \Sigma_{xx} & \Sigma_{xy} \\ \Sigma_{yx} & \Sigma_{yy} \end{matrix} \right\rbrack = \left\lbrack \begin{matrix} 1 & \Sigma_{xy}\Sigma_{yy}^{-1} \\ 0 & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix} \Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx} & 0 \\ 0 & \Sigma_{yy} \end{matrix} \right\rbrack  \left\lbrack \begin{matrix} 1 & 0 \\ \Sigma_{yy}^{-1} \Sigma_{yx} & 1 \end{matrix} \right\rbrack
$$

矩阵求逆：

$$
\left\lbrack \begin{matrix} \Sigma_{xx} & \Sigma_{xy} \\ \Sigma_{yx} & \Sigma_{yy} \end{matrix} \right\rbrack ^{-1} = \left\lbrack \begin{matrix} 1 & 0 \\ -\Sigma_{yy}^{-1}\Sigma_{yx} & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix} (\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})^{-1} & 0 \\ 0 & \Sigma_{yy}^{-1} \end{matrix} \right\rbrack \left\lbrack \begin{matrix} 1 & -\Sigma_{xy}\Sigma_{yy}^{-1} \\ 0 & 1 \end{matrix} \right\rbrack
$$

这时，重写联合概率密度函数的指数部分：

$$
\begin{align}
& \left( \left\lbrack \begin{matrix} x \\ y \end{matrix} \right\rbrack -  \left\lbrack \begin{matrix} \mu_x \\ \mu_y \end{matrix} \right\rbrack \right)^T \left\lbrack \begin{matrix} \Sigma_{xx} & \Sigma_{xy} \\ \Sigma_{yx} & \Sigma_{yy} \end{matrix} \right\rbrack ^{-1} \left( \left\lbrack \begin{matrix} x \\ y \end{matrix} \right\rbrack -  \left\lbrack \begin{matrix} \mu_x \\ \mu_y \end{matrix} \right\rbrack \right) \\ 
= &\left( \left\lbrack \begin{matrix} x \\ y \end{matrix} \right\rbrack -  \left\lbrack \begin{matrix} \mu_x \\ \mu_y \end{matrix} \right\rbrack \right)^T \left\lbrack \begin{matrix} 1 & \Sigma_{xy}\Sigma_{yy}^{-1} \\ 0 & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix} \Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx} & 0 \\ 0 & \Sigma_{yy} \end{matrix} \right\rbrack  \left\lbrack \begin{matrix} 1 & 0 \\ \Sigma_{yy}^{-1} \Sigma_{yx} & 1 \end{matrix} \right\rbrack \left( \left\lbrack \begin{matrix} x \\ y \end{matrix} \right\rbrack -  \left\lbrack \begin{matrix} \mu_x \\ \mu_y \end{matrix} \right\rbrack \right) \\
= &(x - \mu_x - \Sigma_{yy}^{-1}\Sigma_{yx}(y-\mu_y) )^T  (\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx} )^{-1}  (x - \mu_x - \Sigma_{yy}^{-1}\Sigma_{yx}(y-\mu_y) ) + (y - \mu_y)^T \Sigma_{yy}^{-1} (y - \mu_y)
\end{align}
$$

等式的第二部分正好是 $y$ 的概率密度函数的指数部分，又由条件概率公式 $p(x,y) = p(x \vert y) p(y)$ 可以得出，等式的第一部分应指条件概率 $p(x \vert y)$ 的指数部分。此时，我们可以得出以下结论：

$$
\begin{align}
& p[x \vert y] = \mathcal{N} (\mu_x + \Sigma_{yy}^{-1}\Sigma_{yx}(y-\mu_y),\; \Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})  \tag{6.3.2} \\
& p(y) = \mathcal{N} (\mu_y ,\; \Sigma_{yy}) \tag{6.3.3}
\end{align}
$$


#### 6.4 矩阵迹的求导



对于一矩阵 $A \in \mathbb{R}^{n \times n }$ ，每一列表示为 $a_1, \dots , a_n$ ， 每一行的形式表示为 $\tilde{a}_1^T \dots , \tilde{a}_n^T$， 则有：

$$
\frac{\partial tr XA}{\partial X} = \frac{\partial tr \left\lbrack \begin{matrix} d\tilde{x}_i^T a_i & & \\ & \ddots & \\ & & d\tilde{x}_n^T a_n \end{matrix} \right\rbrack}{\partial X} = \frac{\sum_{j=1}^n d\tilde{x}_j^T a_j }{dX}
$$

接着，我们有：

$$
\left\lbrack \frac{\partial tr XA}{\partial X}  \right\rbrack _{ij} = \left\lbrack \frac{\sum_{j=1}^n d\tilde{x}_j^T a_j }{\partial x_{ji}}  \right\rbrack = a_{ji}
$$

所以：

$$
\frac{\partial tr XA}{\partial X} \equiv A^T \tag{6.4.1}
$$

令 $f(X) = XA$， 有： 

$$
\begin{align}
\frac{\partial tr X A X^T}{\partial X} & = \nabla_X tr f(X) X^T \\ 
& = \nabla_{\bullet} f(\bullet) X^T +  \nabla_{\bullet}tr f(A) \bullet^T \\
& = X f'(\bullet) + (\nabla_{\bullet^T}tr f(A) \bullet^T )^T \\
& = XA^T + (\nabla_{\bullet^T}tr\bullet^T f(A)  )^T \\
& = XA^T + (f(A)^T)^T \\
& = XA^T + XA \tag{6.4.2}
\end{align}
$$














## 参考

1. State Estimation for Robotics, Timothy D. BARFOOT
2. 视觉SLAM十四讲, 高翔 
3. Properties of the Trace and Matrix Derivatives, John Duchi, Stanford





[1]:https://res.cloudinary.com/bxy1994/image/upload/v1553094985/SLAM/KF_scheme.png
[2]:https://res.cloudinary.com/bxy1994/image/upload/v1553001904/SLAM/KF.png
